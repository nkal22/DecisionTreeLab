---
title: "In Class DT"
author: "Brian Wright"
date: "December 7, 2017"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(rio)
library(plyr)
library(tidyverse)
library(rpart)
library(psych)
library(pROC)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("rattle")
library(rattle)
library(caret)
setwd("/cloud/project/decision_trees")

```

Congrats! You just graduated from medical school and got a PhD in Data Science at the same time, wow impressive. Because of these incredible accomplishments the world now believes you will be able to cure cancer...no pressure. To start you figured you better create some way to detect cancer when present. Luckily because you are now a MD and DS PhD or MDSDPhD, you have access to data sets and know your way around a ML classifier. So, on the way to fulfilling your destiny to rig the world of cancer you start by building several classifiers that can be used to aid in determining if patients have cancer and the type of tumor. 

The included dataset (clinical_data_breast_cancer_modified.csv) has information on 105 patients across 20 variables, your goal is to build two classifiers one for PR.Status (progesterone receptor), a biomarker that routinely leads to a cancer diagnosis, indicating if there was a positive or negative outcome and one for the Tumor multi-class variable . You would like to be able to explain the model to the mere mortals around you but need a fairly robust and flexible approach so you've chosen to use decision trees to get started and will possibly move to a ensemble model if needed. 

In doing so, similar to  great data scientists working at very average marketing firms, you remembered the excellent education provided to you at UVA in a undergrad data science course and have outline 20ish steps that will need to be undertaken to complete this task (you can add more or combine if needed).  As always, you will need to make sure to #comment your work heavily and render the results in a clear report (knitted) as the non MDSDPhDs of the world will someday need to understand the wonder and spectacle that will be your R code. Good luck and the world thanks you. 

 Footnotes: 
-	Some of the steps will not need to be repeated for the second model, use your judgment
-	You can add or combine steps if needed
-	Also, remember to try several methods during evaluation and always be mindful of how the model will be used in practice.   


```{r}
#1 Load the data and ensure the column names don't have spaces, hint check.names. 
tree_1 <- tibble(import("clinical_breast_cleaned.csv", check.names= TRUE))

tree_1 <- tree_1 %>% select (-c(ER.Status))




describe(tree_1)
View(tree_1)
str(tree_1)
```

```{r}
#2 Ensure all the variables are classified correctly and ensure the target variable for "PR.Status" is 0 for negative and 1 for positive

#It does
```

```{r}
#3 Don't check for correlated variables....because it doesn't matter with Decision Trees...that was easy
```


```{r}
#4 You also don't need to complete a test train split because the rpart defaults to 10 fold cross-validation to train the model...you're welcome. (You can certainly build trees with a test and train)

```

```{r}

#5 Guess what, you also don't need to standardize the data, because DTs don't give a ish, they make local decisions...keeps getting easier 

```

```{r}
#6 Ok now determine the base rate for the classifier, what does this number mean.  For the multi-class this will be the individual percentages for each class.

sum(tree_1$PR.Status)
length(tree_1$PR.Status)

(x <- 1- sum(tree_1$PR.Status)/length(tree_1$PR.Status)) #0.49

```

```{r}
#7 Build your model using the default settings

tree_1_long = tree_1 %>% gather(Var, #<- list of predictor variables
                                Value,#<- the values of those predictor variables
                                -PR.Status)  #<- the column to gather the data by

View(tree_1_long)

tree_1_long_form = ddply(tree_1_long, 
                            .(Var, Value),#<- group by Var and Value, "." allows us to call the variables without quoting
                            summarize,  
                            prob_cancer = mean(PR.Status), #<- probability of being pregnant
                            prob_not_cancer = 1 - mean(PR.Status)) #<- probability of not being pregnant

View(tree_1_long_form)

#tree_1 = lapply(tree_1, function(x) as.factor(x))

str(tree_1)

tree_1 <- as_tibble(tree_1)

table(tree_1$PR.Status)

tree_1

tree_1$PR.Status <- factor(tree_1$PR.Status,labels = c("not_canc", "canc"))

#set.seed(1980)
tree_1_tree_gini = rpart(PR.Status~.,  #<- formula, response variable ~ predictors
                           #   "." means "use all other variables in data"
                            method = "class",#<- specify method, use "class" for tree
                            parms = list(split = "gini"),#<- method for choosing tree split
                            data = tree_1,#<- data used
                            control = rpart.control(cp=.001))

#Look at the results
tree_1_tree_gini$variable.importance




```

```{r}
#8 View the results, what is the most important variable for the tree?
View(tree_1_tree_gini$frame)
```

```{r}
#9 Plot the tree using the rpart.plot package
pdf("DecisionTreeDiagram.pdf")
rpart.plot(tree_1_tree_gini, type =4, extra = 101)
dev.off()
```


```{r}
#10 plot and convert the cp table to a data.frame
View(tree_1_tree_gini$cptable)

cptable_1 <- as_tibble(tree_1_tree_gini$cptable, )
str(cptable_1)


```

```{r}
#11 Add together the real error and standard error to create a new column and determine the optimal number of splits.
cptable_1$opt <- cptable_1$`rel error`+ cptable_1$xstd

cptable_1

plotcp(tree_1_tree_gini)
```


```{r}
#12 Use the predict function and your model to predict the target variable.

tree_1_fitted_model = predict(tree_1_tree_gini, type= "class")

View(as.data.frame(tree_1_fitted_model))
```

```{r}
#13 Compare the predicted values to those of the actual by generating a matrix ("by-hand").
canc_conf_matrix = table(tree_1_fitted_model, tree_1$PR.Status)
canc_conf_matrix

table(tree_1_fitted_model)

confusionMatrix(as.factor(tree_1_fitted_model), as.factor(tree_1$PR.Status), positive = "canc", dnn=c("Prediction", "Actual"), mode = "sens_spec")

table(tree_1$PR.Status)
```

```{r}
#14 Generate, "by-hand", the hit rate and detection rate and compare the detection rate to your original baseline rate. How did your model work?

#Not Well
sum(canc_conf_matrix[row(canc_conf_matrix)!= col(canc_conf_matrix)]) #27
sum(canc_conf_matrix)

canc_error_rate = sum(canc_conf_matrix[row(canc_conf_matrix) != col(canc_conf_matrix)]) / sum(canc_conf_matrix)

paste0("Hit Rate/True Error Rate:", canc_error_rate * 100, "%")
#25.71%

canc_conf_matrix[2,2]/sum(canc_conf_matrix) #35.24%

(2*0.6852*.7872)/(0.6852+.7872)


```

```{r}
#15 Use the the confusion matrix function to check a variety of metrics and comment on the metric that might be best for this type of analysis.  

#Detection Rate, Sensitivity, Accuracy
```

```{r}
#16 Generate a ROC and AUC output, interpret the results
canc_roc <- roc(tree_1$PR.Status, as.numeric(tree_1_fitted_model), plot = TRUE)


tree_1_fitted_prob = predict(tree_1_tree_gini, type= "prob")
View(tree_1_fitted_prob)

roc(tree_1$PR.Status, ifelse(tree_1_fitted_prob[,'not_canc'] >= .75,0,1), plot=TRUE)
```
Another useful metric uses an ROC (receiver operating curve) which plots the sensitivity versus specificity at varying cutoff thresholds (the probabilistic threshold the model uses to classify a case as positive). An AUC (area under curve) value is calculated based on the ROC, and the value for this model is 0.7309 which is a fair rating as we want the value to be >0.8.
```{r}
#17 Use the predict function to generate percentages, then select several different threshold levels using the confusion matrix function and interpret the results? What metric should we be trying to optimize. 
```

```{r}
#18 Use your optimal cp (from step 11) (assuming it's different) and rerun the model, how does this impact the quality of the model. 
```

```{r}
#19 Try adjusting several other hyperparameters via rpart.control and review the model evaluation metrics. 
```

```{r}
#20 Follow the same steps for the multi-class target, tumor, aside from step 1, 2 and 14. For step 15 compare to the four base rates and see how you did. 

tree_2 <- tibble(import("clinical_breast_cleaned.csv", check.names= TRUE))

tree_2 <- tree_2 %>% select (-c(ER.Status))




describe(tree_2)
View(tree_2)
str(tree_2)
```

```{r}
#21 Summarize what you learned for each model along the way and make recommendations to the world on how this could be used moving forward, being careful not to over promise. 
```



